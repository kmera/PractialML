---
title: "Human Activity Quality in Weight Lifting Exercise"
author: "Klever Mera"
output: html_document
---
## Overview
This project consists on predicting the Quality of Human Activity when a person develops weight lifting exercises. The datasets were obtained considering 6 participants who performed the exercises correctly and incorrectly in 5 different ways. 

```{R include = FALSE}
library(data.table)
library(caret)
library(dplyr)
library(ggplot2)
library(parallel)
library(doParallel)
library(scales)
```

## Loading and Processing the Raw Data
First, training and testing csv files will be downloadad, and then work with the training file to create both the training dataset and the test dataset respectivly. The training partition will be 75% of the entire dataset and 25% to the test part which will be done by createDataPartition command included in the 'caret' package. The testing csv file wonâ€™t be used till the time to work with the test case and the seed was set to 998.
```{R cache = TRUE, include = FALSE}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_file <- fread(url_train)
test_file <- fread(url_test)
train_file <- as.data.frame(train_file)
test_file <- as.data.frame(test_file)

set.seed(998)
inTrain <- createDataPartition(train_file$classe, p = .75, list = F)
training <- train_file[inTrain, ]
testing <- train_file[-inTrain, ]
```

### Training
Training csv file has 19622 observations and 160 variables. The 'classe' variable is the one that will be used as a predicted one, and this variable has 5 levels, which are: "exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes." Read more: http://groupware.les.inf.puc-rio.br/har#ixzz5u2aO6IXV

```{R}
dim(train_file)
```
```{R, include = FALSE}
fig1 <- train_file %>% ggplot(aes(classe, fill = classe)) + 
        geom_bar() + 
        xlab("classe variable") +
        ggtitle("Fig 1. classe variable distribution")
```

The Figure 1 shows the distribution of the 5 Classes (A, B, C, D, and E) of the classe variable in the training csv file. Class A has more quantity, which means the participants were doing the exercise in the correct way.
```{R}
fig1
```

The first review of the training dataset is the proportion of the missing values:
```{R}
percent(mean(is.na(training)))
```

The proportion of the missing values is high (60%+), so a strategy is needed to deal with. In this case, the columns with high rate of missing values won't be considered. Besides, the first seven columns didn't provide information to take into account for prediction, so those columns will also be eliminated.   
```{R, include = FALSE}
NA.data <- function(x) {
        y <- sum(is.na(x))/nrow(training)
}

rate_na <- sapply(training, NA.data)
new_file <- training[!(rate_na > .95)]

new_training <- new_file[c(8:60)]
new_training$classe <- as.factor(new_training$classe)
```

The new proportion of missing values is really low and the new data set has 53 columns now, those variables (except classe) will be considered as predictors.
```{R}
percent(mean(is.na(new_training)))
dim(new_training)
```

### Testing
The same approaches considering for the training dataset will be applied for the testing dataset, so the same strategy for missing values and for the first seven columns.
```{R include = FALSE}
NA.data <- function(x) {
        y <- sum(is.na(x))/nrow(testing)
}

rate_na <- sapply(testing, NA.data)
new_file <- testing[!(rate_na > .95)]

new_testing <- new_file[c(8:60)]
new_testing$classe <- as.factor(new_testing$classe)
```

The training dataset has been cleaned.
```{R}
percent(mean(is.na(new_testing)))
dim(new_testing)
```
## Modeling
Based on the power of Random Forest as an ensembling machine learning algorithm this is the model that will be used to predict, so in the train command of caret package the method will be "rf". But before that, some tuning parameters should be considered such as type of resampling, in this case repeated (10 times) 10-fold cross-validation has been defined for assessing model performance; moreover, and in order to speed up the process, the option mtry() was considered, which basically refers to the number of variables available for splitting at each tree node, so in this case mtry was set to 7. Finally, paralell processing was also considered to improve Random Forest performance.
```{R cache = TRUE}
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

fitControlRF <- trainControl(method = "repeatedcv",
                             number = 10,
                             repeats = 10,
                             allowParallel = TRUE)

modFitRF <- train(classe ~ ., data = new_training, method = "rf", 
                   trControl = fitControlRF, 
                           tuneGrid=data.frame(mtry = 7))

stopCluster(cluster)
registerDoSEQ()
print(modFitRF)
```

## Confusion Matrix
The confusion matrix is useful to compare model performance. If the diagonal values are high and all other values are low, the desired classes are being predicted correctly.

```{R}
predRF <- predict(modFitRF, new_testing)
confmatRF <- confusionMatrix(predRF, new_testing$classe)
confmatRF
```
As we can see, the diagonal values are high and just a few and low false positives and false negatives, so the model has done a good work with the prediction. Furthermore, the Accuracy of the model is 0.9947818, so expected out of sample error is 0.0052.

## Test case
Finally, it's time to use the testing csv file in order to test the model. This file has only 20 observations and the variable classe isn't present. As was done for training and testing dataset, the strategy to manage missing values will be applied and also the first seven columns will be eliminated.

```{R, include = FALSE}
NA.data <- function(x) {
        y <- sum(is.na(x))/nrow(test_file)
}

rate_na <- sapply(test_file, NA.data)
new_file <- test_file[!(rate_na > .95)]

testing20 <- new_file[c(8:60)]
percent(mean(is.na(testing20)))
```
```{R}
predRF2 <- predict(modFitRF, testing20)
testing20$classe <- predRF2
testing20$classe
```

## Conclusions
In the raw data there were plenty of missing values, so a procedure to manage them was considered and in that way many columns were eliminated. Besides, the first seven columns were eliminated because the information, in this case, wasn't useful for the prediction model, for instance 'user_name'. Pre-procesing options such as PCA wasn't also considered.

Random Forest was the only prediction model considered because during the definition model was also considered Boosting but the accuracy was low and when the model combination was applied the accuracy was the same as the Random Forest, so only this model was used.

Random Forest method ("rf") in the caret package consumes a lot of computer resources, so parallel processing technique was also included in the model in order to speed up the process.

The confusion matrix shows the accuracy which in this case is 0.9965. The Precision/Recall and F1 Score could also be included in a next analysis as a key metrics for a complete model evaluation.

